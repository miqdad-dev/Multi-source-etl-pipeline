# ğŸ› ï¸ Multi-Source ETL Pipeline

This project demonstrates a complete **ETL (Extract, Transform, Load)** pipeline built in Python that ingests data from multiple file formats â€” **CSV, JSON, and XML** â€” transforms it, and writes it to a unified output CSV file.

---

## ğŸ“Œ Project Features

- âœ… Automatically detects and extracts data from:
  - `.csv` files using `pandas.read_csv()`
  - `.json` files using `pandas.read_json()`
  - `.xml` files using `ElementTree`
- âœ… Transforms:
  - Height from inches â†’ meters
  - Weight from pounds â†’ kilograms
- âœ… Combines all data into one dataset
- âœ… Saves the transformed output to `transformed_data.csv`
- âœ… Logs every stage to `log_file.txt` for traceability

---

## ğŸ§° Tech Stack

- Python 3
- `pandas`
- `glob`
- `datetime`
- `xml.etree.ElementTree`

---

## ğŸ“ Folder Structure

```
ETL folder/
â”œâ”€â”€ etl.py                  # Main ETL pipeline script
â”œâ”€â”€ log_file.txt            # Log of ETL events
â”œâ”€â”€ transformed_data.csv    # Final output file
â”œâ”€â”€ source1.csv/json/xml    # Input data files
â”œâ”€â”€ source2.csv/json/xml
â”œâ”€â”€ source3.csv/json/xml
```

---

## ğŸš€ How to Run

1. Clone the repository
2. Navigate to the `ETL folder/`
3. Run the script:

```bash
python etl.py
```

This will process all source files and generate:
- `transformed_data.csv`
- `log_file.txt`

---

## ğŸ” Example Output (CSV)

| name  | height | weight |
|-------|--------|--------|
| John  | 1.80   | 75.30  |
| Alice | 1.65   | 60.12  |

---

## ğŸ‘¨â€ğŸ’» Author

**Miqdad Issa**  
Aspiring Data Engineer | Project-driven learner | Open Source Contributor  
[github.com/miqdad-dev](https://github.com/miqdad-dev)